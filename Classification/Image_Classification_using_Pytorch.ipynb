{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification_practice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "04S2MWOX3vJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "a7a732d9-4a13-46ff-a36d-58f1d26f4d9e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 12 20:35:10 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut1uDoXJzCTu",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification using PyTorch in 2020  \n",
        "\n",
        "In this notebook we will be utilizing some of the latest advancements in the  \n",
        "[PyTorch Ecosystem](https://pytorch.org/ecosystem/ \"Click to visit the PyTorch Ecosystem homepage\") to build a simple image classifier using CNNs.   \n",
        "\n",
        "Along the way, we will learn some PyTorch and CNN (Convolution Neural  \n",
        "Networks) basics.   \n",
        "\n",
        "<br/>Note: You can find this notebook along with the master notebook (with  \n",
        "all the code) in this Github Repository,  \n",
        "https://github.com/pranjalchaubey/Deep-Learning-Notes\n",
        "\n",
        "Please checkout the `PyTorch Image Classification in 2020` folder. \t "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFhLAYud0CQK",
        "colab_type": "text"
      },
      "source": [
        "### 1. Get the Dataset Onboard\n",
        "\n",
        "In any Machine Learning/Data Science problem, the first step is always to get  \n",
        "the dataset.  \n",
        "\n",
        "In our case, to get things started, we will initially use the simple [MNIST Dataset](https://en.wikipedia.org/wiki/MNIST_database \"Wikipedia to the rescue!\").  \n",
        "MNIST is largely considered the _'Hello World!'_ of AI/ML. The dataset was  \n",
        "created way back in the late 90s. The [official description](http://yann.lecun.com/exdb/mnist/ \"Yann Lecun is God.\") states,  \n",
        "\n",
        "_\"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image._  \n",
        "\n",
        "_It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\"_  \n",
        "\n",
        "<br/>You might be wondering, how to get this dataset in our Colab Workspace?  \n",
        "PyTorch comes with a _datasets_ module called, [Torchvision.Datasets](https://pytorch.org/docs/stable/torchvision/datasets.html \"Official Documentation\").  \n",
        "Torchvision.Datasets module contains a number of publically available datasets  \n",
        "including the one we are looking for, MNIST. You are encouraged to explore the  \n",
        "Torchvision.Datasets documentation page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tia9JXE46rJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets import some libraries \n",
        "import torch # PyTorch \n",
        "from torchvision import datasets # Datasets module \n",
        "import torchvision.transforms as transforms # Image Transforms \n",
        "from torch.utils.data.sampler import SubsetRandomSampler # Sampler "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVP4itgp7jC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Data Science Regulars\n",
        "# ---- FILL IN ----\n",
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h8vdeaZ7sj_",
        "colab_type": "text"
      },
      "source": [
        "Checking out the torchvision.datasets module documentation, we find  \n",
        "![Torchvision.Dataset](https://drive.google.com/uc?id=1Zsgc5_PnO9BQQ5wqssf67A5Ge-qIXtLh)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UapF3_qS67Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert data to torch.FloatTensor(transforming images into tensors)\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True, download=True,\n",
        "                            transform=transform) # ---- FILL IN ----\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, \n",
        "                          transform=transform) # ---- FILL IN ----"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnZrOAcMZRTp",
        "colab_type": "text"
      },
      "source": [
        "### 2. Train Validation Test Split \n",
        "\n",
        "Once the download is complete (usually instantaneous), you should be able to  \n",
        "see the MNIST dataset downloaded inside the _'data'_ folder on the left hand  \n",
        "side. (Click on the _Files_ icon on the left sidebar)  \n",
        "\n",
        "We have both the training and the test sets. Now we need to bifurcate the   \n",
        "training set in two parts,  \n",
        "1. Training Set (80% images)\n",
        "2. Validation Set (20% images)  \n",
        "\n",
        "The algorithm we use to do this is quite simple,  \n",
        "1. Create a list of indices of the training data \n",
        "2. Randomly Shuffle those indices \n",
        "3. Slice the indices in 80-20 split \n",
        "\n",
        "[Why create a _Validation Set_ at all?](https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set \"In order to avoid overfitting on the test set!\") "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrsPBIfRYwNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "deb7a9eb-1d13-4beb-cdcf-86c8a7187290"
      },
      "source": [
        "# obtain training indices that will be used for validation\n",
        "\n",
        "# 1. Create a list of indices of the training data  \n",
        "# ---- FILL IN ----\n",
        "num_train = len(train_data)\n",
        "print('num_train', num_train)\n",
        "indices = list(range(num_train))\n",
        "print(len(indices))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_train 60000\n",
            "60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXwkT_bS5xfR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "92d40e28-8c91-4fba-944d-85d423d8675e"
      },
      "source": [
        "print(range.__doc__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(stop) -> range object\n",
            "range(start, stop[, step]) -> range object\n",
            "\n",
            "Return an object that produces a sequence of integers from start (inclusive)\n",
            "to stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\n",
            "start defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\n",
            "These are exactly the valid indices for a list of 4 elements.\n",
            "When step is given, it specifies the increment (or decrement).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjDu_dd35TZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Randomly Shuffle those indices\n",
        "# ---- FILL IN ----\n",
        "np.random.shuffle(indices)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKqrNSg-5VMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b7776fba-372a-4148-a172-12a0862f0a30"
      },
      "source": [
        "# 3. Slice the indices in 80-20 split\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2 # ie Train Set divided into two parts \n",
        "                 # 80% Train 20% Validation \n",
        "# ---- FILL IN ----\n",
        "split = int(np.floor(valid_size*num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "print(len(train_idx))\n",
        "print(len(valid_idx))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48000\n",
            "12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14RHH0Jt8_k_",
        "colab_type": "text"
      },
      "source": [
        "Please Note that so far we have just been fiddling around with the _'indices'_,  \n",
        "not the actual images as such.....but Why?  \n",
        "Answer below.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlTpuhxVAfCK",
        "colab_type": "text"
      },
      "source": [
        "### 3. Prepare the Dataloaders \n",
        "\n",
        "We have downloaded the dataset, and created a train/valid/test split.  \n",
        "Q: How do we _'push'_ this data into a PyTorch model?  \n",
        "A: PyTorch has a mechanism to _'ingest'_ data from a dataset through a module  \n",
        "known as `DataLoader`.  \n",
        "\n",
        "A great analogy,  \n",
        "\n",
        "![DataLoader](https://drive.google.com/uc?id=1U4IG-5lbFGQQS4xwQPU2QiYdR1hFGBZ5 \"Always remember, Deep Learning Model is Your Baby and you got to feed it well!\")\n",
        "\n",
        "[Great tutorial on DataLoaders.](https://www.journaldev.com/36576/pytorch-dataloader \"PyTorch DataLoader\")  \n",
        "[Ultimate tutorial on DataLoaders.](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel \"A detailed example of how to generate your data in parallel with PyTorch\") \n",
        "\n",
        "Time to prepare the _DataLoaders_ now!  \n",
        "\n",
        "![DataLoader Documentation](https://drive.google.com/uc?id=1YFbWIGwNlL5Kp4Zvt52Ck0_Wk4MNfxS9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Zzb55d8k7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define samplers for obtaining training and validation batches\n",
        "# remember train_idx and valid_idx were the indices that we shuffled above\n",
        "# ---- FILL IN ----\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare dataloaders\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0 # do not modify \n",
        "# how many samples per batch to load\n",
        "batch_size = 20 # ie 20 images per batch \n",
        "\n",
        "# Training Set \n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, \\\n",
        "                                           batch_size=batch_size, \\\n",
        "                                           sampler=train_sampler, \\\n",
        "                                           num_workers=num_workers)\n",
        "# Validation Set \n",
        "#valid_loader = # ---- FILL IN ----\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=train_data, \\\n",
        "                                           batch_size=batch_size, \\\n",
        "                                           sampler=valid_sampler, \\\n",
        "                                           num_workers=num_workers)\n",
        "\n",
        "# Test Set \n",
        "# Notice we have not used a 'sampler' here as it was not required \n",
        "#test_loader = # ---- FILL IN ----\n",
        "test_loader = torch.utils.data.DataLoader(dataset=train_data, \\\n",
        "                                           batch_size=batch_size, \\\n",
        "                                           num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBy_7mS_4hdc",
        "colab_type": "text"
      },
      "source": [
        "We got the dataloaders working, but how do we know that they are working indeed?  \n",
        "Visualizing the data from the dataloaders would be a good check! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrRjUiQG5uOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e32babbf-2696-434e-ed60-9a5f1814d377"
      },
      "source": [
        "# Visualize a whole batch of data from the dataloaders \n",
        "\n",
        "# ---- FILL IN ----\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "#print(iter.__doc__)\n",
        "\n",
        "\n",
        "print(len(images), len(labels)) # Should be equal to the batch size, 20\n",
        "print('Correct Labels: ', labels)\n",
        "images = images.numpy() # Convert the images to numpy array for matplotlib\n",
        "print('Shape of our images tensor =', images.shape)\n",
        "print('Batch Size =', images.shape[0], 'Image Height/Width =', \\\n",
        "                                                        images.shape[2])\n",
        "\n",
        "print()\n",
        "print('Squeezing the images tensor =', np.squeeze(images).shape)\n",
        "print('Un-squeezing the images tensor (axis=3) =', \\\n",
        "                                        np.expand_dims(images, axis=3).shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 20\n",
            "Correct Labels:  tensor([4, 3, 4, 8, 7, 5, 8, 2, 7, 7, 8, 3, 1, 2, 9, 1, 8, 3, 5, 6])\n",
            "Shape of our images tensor = (20, 1, 28, 28)\n",
            "Batch Size = 20 Image Height/Width = 28\n",
            "\n",
            "Squeezing the images tensor = (20, 28, 28)\n",
            "Un-squeezing the images tensor (axis=3) = (20, 1, 28, 1, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y58P0-etIck4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "# Plots are plotted inside the notebooks, 'inline'\n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8sSrheGg9-",
        "colab_type": "text"
      },
      "source": [
        "With matplotlib, always remember that _figures contain axes which in turn   \n",
        "contain the plots_.  \n",
        "![Real Python](https://drive.google.com/uc?id=1KdlAGoCK8Lj9pFkrZf52oqOJK3sH3JuH \"Figure Contains the Axes Contains the Axis Contains the Plot\")  \n",
        "\n",
        "[Great tutorial on Matplotlib.](https://realpython.com/python-matplotlib-guide/ \"Real Python has some of the best Python Tutorials on the Internet...no kidding!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC5KTKSv6Y8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "0fbf5824-1ef8-4b99-8e29-725b263c2790"
      },
      "source": [
        "# Plot the whole batch \n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# Loop over all the images in the batch(20)\n",
        "for idx in np.arange(20):\n",
        "    # Add a subplot for the image \n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    # Populate the subplot with the image \n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    # print out the correct label for each image\n",
        "    # .item() gets the value contained in a Tensor\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls_AFh0FzMkG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "f7a2f9c8-5e75-4770-b5fe-cc1abf56d742"
      },
      "source": [
        "# A neat little numpy trick\n",
        "# Just for fun! \n",
        "\n",
        "dataiter = iter(train_loader)  \n",
        "images, labels = dataiter.next() \n",
        "images = images.numpy() \n",
        "np.set_printoptions(precision=2, threshold=None, edgeitems=None, \\\n",
        "                    linewidth=180, suppress=None)\n",
        "print('Label', labels[0])\n",
        "print(images[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label tensor(5)\n",
            "[[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.35 0.58 1.   0.67 0.19 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.38 0.33 0.   0.11 0.84 0.99 0.99 0.99 0.38 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.56 0.95 0.98 0.98 0.95 0.96 0.99 0.99 0.85 0.05 0.02 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.56 0.98 0.99 0.99 0.99 0.78 0.99 0.7  0.23 0.06 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.89 0.99 0.99 0.71 0.1  0.05 0.1  0.03 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.89 0.99 0.87 0.18 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.54 0.58 0.98 0.99 0.49 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.38 0.98 0.99 0.99 0.99 0.15 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.46 0.99 0.99 0.99 0.71 0.03 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.35 0.98 0.99 0.99 0.37 0.22 0.26 0.26 0.26 0.26 0.62 0.77 0.43 0.22 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.56 0.99 0.99 0.9  0.96 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.96 0.74 0.7  0.1  0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.35 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.84 0.93 0.99 0.99 0.99 0.99 0.81 0.09 0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.28 0.5  0.81 0.91 0.65 0.31 0.31 0.31 0.09 0.21 0.41 0.99 0.99 0.99 0.99 0.46 0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.17 0.   0.   0.   0.   0.   0.   0.16 0.99 0.99 0.99 0.99 0.46 0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02 0.53 0.99 0.99 0.99 0.99 0.46 0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.24 0.99 0.99 0.99 0.99 0.99 0.46 0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.43 0.87 0.99 0.99 0.99 0.99 0.99 0.42 0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02 0.05 0.17 0.87 0.99 0.99 0.96 0.97 0.64 0.41 0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.29 0.45 0.65 0.99 0.99 0.99 0.62 0.46 0.16 0.24 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.41 0.48 0.24 0.75 0.86 0.62 0.15 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2APl70a5LH66",
        "colab_type": "text"
      },
      "source": [
        "Our dataloaders seem to be working fine and out data looks great!  \n",
        "<br/> \n",
        "Time to build our CNN based image classification model in PyTorch.....in 2020! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGFnKhH5Lh9t",
        "colab_type": "text"
      },
      "source": [
        "### 4. Build a PyTorch CNN Model\n",
        "But first, we need to know how CNNs work and what are the components of a  \n",
        "typical CNN based image classification architecture.  \n",
        "\n",
        "![CNN Scan](https://media.giphy.com/media/vRINohj6YtmnkTQqHi/giphy.gif \"'Bad Guy' uses Tensorflow :P\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e99lxoYpt9o8",
        "colab_type": "text"
      },
      "source": [
        "####4.1.1 CNN Architecture  \n",
        "A typical CNN Architecture looks like this,  \n",
        "\n",
        "![Block CNN Architecture](https://drive.google.com/uc?id=1RYYh27hsyY5Mx4L_lwGUVGQ3OYkSCqB- \"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\")  \n",
        "<br/>A real life example of VGG-16,  \n",
        "\n",
        "![VGG-16 Architecture](https://drive.google.com/uc?id=14OYg0ihFKHGsLQLcqoeSLt2nyPS4quss \"https://www.researchgate.net/figure/The-architecture-of-a-VGG-16-network_fig2_330467052\")  \n",
        "<br/>Another example, very close to what we are going to build today,  \n",
        "\n",
        "![MNIST CNN Architecture](https://drive.google.com/uc?id=1cnDzorKeRmNUAUJYw2yvQiY2D4f-sDNL \"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obnRoku91VXj",
        "colab_type": "text"
      },
      "source": [
        "####4.1.2 Basic CNN Layer  \n",
        "So how does a CNN layer actually _'works'_?  \n",
        "\n",
        "![CNN Scan](https://drive.google.com/uc?id=1JictobCPmaIX_9pm2mQewc4QiAzl0pzO \"Remember Billie Eilish above?\")  \n",
        "\n",
        "<br/>A colored image has 3 channels,  \n",
        "![RGB Image](https://drive.google.com/uc?id=1QlU04TZ6IN2IRqQJFB8m6GFJ2MmqYGhz \"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\")  \n",
        "\n",
        "<br/>A 3-channel convolution,  \n",
        "![RGB Convolution](https://drive.google.com/uc?id=1CGIqxGHjJGXr8aduPtwGd1-ikgA3WqJ5 \"https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\")  \n",
        "\n",
        "![RGB Convolution Summation](https://drive.google.com/uc?id=1fpM3NLvyjeiKF6_at0Nv4XhYPRkwH1IY \"https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsM9p5pN55Ht",
        "colab_type": "text"
      },
      "source": [
        "####4.1.3 Stride  \n",
        "_'Stride'_ of the kernel while scanning the image.  \n",
        "\n",
        "Here's an example with _stride=1_,  \n",
        "![CNN Stride = 1](https://drive.google.com/uc?id=1wJd7VCYfiMDes0Ex0SlBL97yBP6P7ajt \"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\")  \n",
        "\n",
        "<br/>Another example with _stride=2_,  \n",
        "![CNN Stride = 2](https://drive.google.com/uc?id=1dG2i4WtxUzu9Wlsl3jyH4ZXxUSYVdKfk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0UTagwYo92ZK"
      },
      "source": [
        "####4.1.4 Padding  \n",
        "Padding ensures that there is no loss of information while an image with a  \n",
        "convolutional kernel.  \n",
        "![Padding = 1](https://drive.google.com/uc?id=1xIQSJVRtAS7em_E387ZBd6sErjwA7RD6 \"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKK72-_H94bi"
      },
      "source": [
        "####4.1.5 Max Pooling  \n",
        "Max Pooling layer primarily reduces the dimensionality of the input.  \n",
        "\n",
        "![Max Pooling](https://drive.google.com/uc?id=11_1ThNaU4e4DAEFs7I9GsNoD7oFmtVES \"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\")  \n",
        "\n",
        "Max Pooling is not the only type of pooling layer out there.  \n",
        "![Type of Pooling](https://drive.google.com/uc?id=1ECZrE8vAhTE1gEsp_3vPy03oLuSj0aOW)  \n",
        "\n",
        "[A nice tutorial on pooling layers.](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/ \"Jason Brownlee is an absolute genius. His blog is a damn goldmine!\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98q9buSu-hxw",
        "colab_type": "text"
      },
      "source": [
        "####4.1.6 Rectified Linear Units aka ReLU  \n",
        "The non-linear activation function.  \n",
        "![ReLU Activation](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png \"https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvnlvaIQ_ECZ",
        "colab_type": "text"
      },
      "source": [
        "####4.1.7 _Fully Connected_ or _Linear Layers_  \n",
        "Final dimensionality reduction for either classification or regression tasks.  \n",
        "\n",
        "![Fully Connected Layers](https://drive.google.com/uc?id=1Fwh-NqMDLx-xqKlqN2c-z780M9aBUHvO \"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xct_yN3R_8pP",
        "colab_type": "text"
      },
      "source": [
        "Before we move forward, a few questions for everyone:  \n",
        "1. What kind of features do the first few CNN layers capture?  \n",
        "2. What kind of features do the last few CNN layers capture? \n",
        "3. What is the role of max-pool?  \n",
        "4. Can we use _stride_ to perform a role similar to max-pool?  \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el1NUdaz0HrE",
        "colab_type": "text"
      },
      "source": [
        "####4.2.1 PyTorch  \n",
        "PyTorch is currently the hottest Deep Learning library out there. In terms of  \n",
        "popularity, it has even taken over Tensorflow. Tensorflow came before PyTorch  \n",
        "and is backed by the engineering and marketing might of _**Google**_.  \n",
        "\n",
        "_Why PyTorch got so darn famous?_  \n",
        "The answer lies in the fact that PyTorch is highly pythonic (due to dynamic  \n",
        "computational graphs) which makes it extremely flexible and ideal for  \n",
        "researchers and developers alike.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iKCm6RQJUL5",
        "colab_type": "text"
      },
      "source": [
        "####4.2.2 Understanding Computational Graphs   \n",
        "At the bottom of every Deep Neural Network training, there are only two things  \n",
        "taking place, \n",
        "1. A forward pass - pushing images/data from the start of the network and   \n",
        "generating an output (and a loss/error).  \n",
        "2. Backpropagation - essentially a backward pass where we calculate gradients  \n",
        "using partial derivatives with respect to the loss, and make changes to the  \n",
        "weights of the network. In a nutshell, this is how deep learning networks  \n",
        "train.  \n",
        "\n",
        "![A Computational Graph](https://drive.google.com/uc?id=1dgVg08M02gfIkPm0JKyhm4k4gYVLB_Me \"Udacity Deep Learning Nano Degree\")  \n",
        "The image above, is a simple neural network. But it is also a   \n",
        "computational graph.  \n",
        "\n",
        "We first make a forward pass through our network and then a backward pass  \n",
        "to calculate how much loss was being contributed by _**W1**_ weight in  \n",
        "particular.  \n",
        "\n",
        "Every neural network you define, PyTorch _sees_ it as a computational graph  \n",
        "similar to what we see above and keeps a track of all the operations   \n",
        "performed by every node. This ensures that it calculates accurate gradients  \n",
        "when make a backward pass.  \n",
        "\n",
        "Good thing about PyTorch is that it creates these computational graphs on the  \n",
        "fly! And this aspect makes PyTorch and extremely felxible (and pythonic) deep  \n",
        "learning library.   \n",
        "\n",
        "![Dynamic Computational Graphs](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/dynamic_graph.gif \"PyTorch building dynamic computational graphs on the fly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNCoUgD7R-wL",
        "colab_type": "text"
      },
      "source": [
        "####4.2.3 Tensors  \n",
        "Tensors are the building blocks of every deep learning library including   \n",
        "PyTorch. What are tensors though?  \n",
        "\n",
        "![Tensors](https://drive.google.com/uc?id=1F5cLclu4RML7zj7axf8IGU7aUXJlI7N0 \"Udacity Deep Learning Nano Degree\")  \n",
        "\n",
        "Creating tensors in PyTorch is easy  \n",
        "```\n",
        "import torch \n",
        "x = torch.rand(3, 3)\n",
        "print(x)\n",
        "\n",
        ">>Prints out:\n",
        ">>tensor([[0.5264, 0.1839, 0.9907],\n",
        ">>        [0.0343, 0.9839, 0.9294],\n",
        ">>        [0.6938, 0.6755, 0.2258]])\n",
        "```\n",
        "Tensors in PyTorch are exactly like the numpy arrays, except that they can   \n",
        "also live on a GPU which makes them realy really fast!  \n",
        "```\n",
        "torch.FloatTensor([[20, 30, 40], [90, 60, 70]]) # Tensor on CPU\n",
        "torch.cuda.FloatTensor([[20, 30, 40], [90, 60, 70]]) # Tensor on GPU\n",
        "```\n",
        "Moving tensors (and complex deep learning models) to a GPU (or a CPU) is   \n",
        "pretty straightforward in PyTorch. \n",
        "```\n",
        "x = torch.FloatTensor([[20, 30, 40], [90, 60, 70]]) # Tensor on CPU\n",
        "print('Is tensor x on GPU?', x.is_cuda) # False\n",
        "x = x.to('cuda') # Moves to GPU\n",
        "print('Is tensor x on GPU?', x.is_cuda) # True \n",
        "x = x.to('cpu') # Moves back to CPU\n",
        "print('Is tensor x on GPU?', x.is_cuda) # False\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0VnwmbZVrLW",
        "colab_type": "text"
      },
      "source": [
        "####4.2.4 The Autograd Module  \n",
        "Autograd is the real rockstar module in PyTorch.  \n",
        "Autograd is the module that keeps a track of all the operations performed on a  \n",
        "tensor and calculates the gradients through a technique called _**Automatic  \n",
        "Differentiation**_.  \n",
        "\n",
        "To enable tracking computation history on a tensor, set _**`.requires_grad`**_  \n",
        "to _**`True`**_. To detach a tensor from its computation history, call   \n",
        "_**`.detach()`**_.  \n",
        "\n",
        "In order to stop autograd from keeping history of computations on a deep  \n",
        "learning model, wrap it around _**`torch.no_grad():`**_. This is usually done  \n",
        "during inference. \n",
        "```\n",
        "with torch.no_grad():\n",
        "    # inference code \n",
        "``` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S0uLBsCZqjG",
        "colab_type": "text"
      },
      "source": [
        "####4.2.5 The nn.Module  \n",
        "The nn module in PyTorch is used to 'build' the neural networks and contains  \n",
        "all the deep learning layers. It obviously depends upon the autograd module to  \n",
        "calculate gradients.  \n",
        "\n",
        "When defining our custom models in PyTorch, we typically subclass the   \n",
        "nn.Module class and override the `__init__()` and `forward()` functions.  \n",
        "1. `__init__()` - This is where we define the layers of our network. \n",
        "2. `forward()` - This is where you actaully connect the layers together and  \n",
        "make everything work.  \n",
        "\n",
        "Don't worry if this sounds a little confusing, we will be seeing `nn.Module`   \n",
        "in action very soon! \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0MkS6OadYRQ",
        "colab_type": "text"
      },
      "source": [
        "####4.2.6 The Optim Package  \n",
        "The optim package in PyTorch contains the optimization algorithms that help   \n",
        "to train your network.  \n",
        "\n",
        "A simple example,  \n",
        "`optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)`  \n",
        "\n",
        "<br/>[My all time favorite 'Intro to PyTorch' tutorial](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e \"Understanding PyTorch with an example: a step-by-step tutorial\")  \n",
        "[Great PyTorch Tutorial Part 1](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/ \"PyTorch 101, Part 1: Understanding Graphs, Automatic Differentiation and Autograd\")  \n",
        "[Great PyTorch Tutorial Part 2](https://blog.paperspace.com/pytorch-101-building-neural-networks/ \"PyTorch 101, Part 2: Building Your First Neural Network\")  \n",
        "[Great PyTorch Tutorial Part 3](https://blog.paperspace.com/pytorch-101-advanced/ \"PyTorch 101, Part 3: Going Deep with PyTorch\")  \n",
        "[Great PyTorch Tutorial Part 4](https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/ \"PyTorch 101, Part 4: Memory Management and Using Multiple GPUs\")  \n",
        "[Great PyTorch Tutorial Part 5](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/ \"PyTorch 101, Part 5: Understanding Hooks\")  \n",
        "[Stunning Insight into the Internals of PyTorch](http://blog.ezyang.com/2019/05/pytorch-internals/ \"PyTorch Internals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYblCGur448H",
        "colab_type": "text"
      },
      "source": [
        "####4.3 Time to define our model!  \n",
        "But before we start off with the model definition, let's have a look at what  \n",
        "the PyTorch Documentation says about the Convolutional NNs.  \n",
        "\n",
        "![Conv2d Layer](https://drive.google.com/uc?id=1odHZIXURYjogjUcyCQ56RYfQPqgzO7DX \"Conv2D\")  \n",
        "\n",
        "<br/>We also need to check out about the MaxPool, Dropout and Linear Layers.  \n",
        "\n",
        "![MaxPool 2D](https://drive.google.com/uc?id=1r1EeLHrV5oAG4OUyRSm0lw2OcZyJQ7ne \"MaxPool2D Layer\")  \n",
        "\n",
        "![Dropout Layer](https://drive.google.com/uc?id=1kYgb4wDrGEBEF5WB169N7Q5hTrwDDO1d \"Dropout Layer\")  \n",
        "\n",
        "![Linear Layer](https://drive.google.com/uc?id=1rrIEqPtun_8Td1js76B2hv22Xm5tjY6m \"Linear Layer\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0zv4wi7BNRL",
        "colab_type": "text"
      },
      "source": [
        "Did you guys notice a weird anomaly in the Conv2d and Linear layers?  \n",
        "```\n",
        "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "```  \n",
        "```\n",
        "torch.nn.Linear(in_features, out_features, bias=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrhOYd8JTQ-B",
        "colab_type": "text"
      },
      "source": [
        "Conv2d layer expects `in_channels` while the Linear layer expects `in_features`.  \n",
        "\n",
        "Bottom line is, that PyTorch expects different things from a tensor dimension.  \n",
        "Specifically,  \n",
        "```\n",
        "\"\"\"Example tensor size outputs, how PyTorch reads them, and where you encounter them in the wild. \n",
        "Note: the values below are only examples. Focus on the rank of the tensor (how many dimensions it has).\"\"\"\n",
        ">>> torch.Size([32])\n",
        "    # 1d: [batch_size] \n",
        "    # use for target labels or predictions.\n",
        ">>> torch.Size([12, 256])\n",
        "    # 2d: [batch_size, num_features (aka: C * H * W)]\n",
        "    # use for as nn.Linear() input.\n",
        ">>> torch.Size([10, 1, 2048])\n",
        "    # 3d: [batch_size, channels, num_features (aka: H * W)]\n",
        "    # when used as nn.Conv1d() input.\n",
        "    # (but [seq_len, batch_size, num_features]\n",
        "    # if feeding an RNN).\n",
        ">>> torch.Size([16, 3, 28, 28])\n",
        "    # 4d: [batch_size, channels, height, width]\n",
        "    # use for as nn.Conv2d() input.\n",
        ">>>  torch.Size([32, 1, 5, 15, 15])\n",
        "    # 5D: [batch_size, channels, depth, height, width]\n",
        "    # use for as nn.Conv3d() input.\n",
        "```    \n",
        "\n",
        "A neat method to make your tensors ready for the linear layer,  \n",
        "```\n",
        "Use view() to change your tensors dimensions.\n",
        "\n",
        "image = image.view(batch_size, -1)\n",
        "\n",
        "You supply your batch_size as the first number, and then -1 basically tells Pytorch, you figure out this other number for me please. \n",
        "Your tensor will now feed properly into any linear layer.\n",
        "```\n",
        "\n",
        "[Incredible Tutorial on PyTorch Layer Dimensions.](https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd \"Eye opener of an article, must read!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouQTXZetn53I",
        "colab_type": "text"
      },
      "source": [
        "Before we start,  \n",
        "1. What is the shape (dimensions) of our images?  \n",
        "2. What is the size of our batch? \n",
        "3. How many _'channels'_ are there in our images?  \n",
        "\n",
        "![Batch Visualization](https://drive.google.com/uc?id=193KFtV2hr-7VkhQUJxd36oCF6Ms12xwD \"Batch Visualization\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-McCK6u3WnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn # nn module contains all the layers \n",
        "import torch.nn.functional as F # same as nn, but a little different "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtUDongrrJlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our CNN based neural architecture \n",
        "# Let's build a simple one with only Convolutional, Linear \n",
        "# and dropout layers\n",
        "class MNISTModel1(nn.Module):\n",
        "    # Here we define the neural architecture \n",
        "    def __init__(self):\n",
        "        super(MNISTModel1, self).__init__() # Initialize the nn module \n",
        "        \n",
        "\n",
        "        # Convolutional Layers\n",
        "        # What shape/dimensions the first layer is going to see? batch_size*channel*image_width*image_height\n",
        "        # Do we need to have some padding for a kernel_size = 3?  \n",
        "        # Input Features = 1 x 28 x 28\n",
        "        # Output Features = ???\n",
        "        # Shape of a Convolutional Layer = (W - K + 2P)\n",
        "        #                                  ------------ + 1\n",
        "        #                                       S\n",
        "        # where, \n",
        "        #       W = Width/Height of previous layer = 28\n",
        "        #       K = Filter Size = 3\n",
        "        #       P = Padding = 0\n",
        "        #       S = Stride = 1(default)\n",
        "        # Therefore, \n",
        "        #           if padding = 0\n",
        "        #           Output Shape = ((28 - 3 + 2*0)/1)+1 = 26 \n",
        "        # We want the dimensions to stay the same so that there is no \n",
        "        # loss of information when performing the convolution. \n",
        "        # Hence, \n",
        "        #       if padding = 1\n",
        "        #       Output Shape = ((28 - 3 + 2*1)/1)+1 = 28\n",
        "        # ---- FILL IN ----\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, \\\n",
        "                               stride=1, padding=1) #8x28x28\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, \\\n",
        "                               stride=1, padding=1) #16x28x28\n",
        "\n",
        "        # Linear Layers \n",
        "        # What shape the first linear layer is going see?\n",
        "        # What are the total number of features given out by conv2?\n",
        "        # Features = 16 x 28 x 28 = 12544\n",
        "        # Therefore,           \n",
        "        # ---- FILL IN ----\n",
        "        self.linear1 = nn.Linear(in_features=12544, out_features=256)\n",
        "        self.linear2 = nn.Linear(in_features=256, out_features=64)\n",
        "        \n",
        "\n",
        "        # Last linear layer should output 10 features as we are \n",
        "        # Classifying the images in 10 categories \n",
        "        # ---- FILL IN ----\n",
        "        self.linear3 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "        # Dropout \n",
        "        # ---- FILL IN ----\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "      \n",
        "\n",
        "    # Here we define the 'forward behaviour' of our neural architecture \n",
        "    def forward(self, image_batch):\n",
        "        # This is also the place where we add ACTIVATION functions \n",
        "        # ---- FILL IN ----\n",
        "        image_batch = F.relu(input=self.conv1(image_batch))\n",
        "        image_batch = F.relu(input=self.conv2(image_batch))\n",
        "\n",
        "        # Remember that when passing image_batch through the Linear layers, \n",
        "        # PyTorch expects: \n",
        "        # >>> torch.Size([12, 256]) -> example values \n",
        "            # 2d: [batch_size, num_features (aka: C * H * W)]\n",
        "            # use for nn.Linear() input.   \n",
        "        # Therefore, we need to 'flatten' image_batch\n",
        "        # image_batch = image_batch.view(batch_size, -1) --> batch size ???\n",
        "        # ---- FILL IN ----\n",
        "        flat_image_batch = image_batch.view(image_batch.shape[0], -1)\n",
        "        flat_image_batch = F.relu(input=self.linear1(flat_image_batch))\n",
        "\n",
        "        # Let's add the dropout too \n",
        "        # ---- FILL IN ----\n",
        "        flat_image_batch = self.dropout(F.relu(input=self.linear2(flat_image_batch)))\n",
        "\n",
        "        # Final Layer of the network \n",
        "        # ---- FILL IN ----\n",
        "        flat_image_batch = F.relu(input=self.linear3(flat_image_batch))\n",
        "\n",
        "        # The output from the final layer is a tensor with 10 'logits'\n",
        "        return flat_image_batch               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPNIgRiS9FN",
        "colab_type": "text"
      },
      "source": [
        "Now that we have defined our model, is there a way we can peep inside to see  \n",
        "what is going on and that if everything is alright?  \n",
        "\n",
        "Say hello to _**[torchsummary !!!](https://github.com/sksq96/pytorch-summary \"Click to visit GitHub Page\")**_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHTtCrNxQ_X7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "773e0b7a-6327-4154-bd51-37d9ef01b6ff"
      },
      "source": [
        "# Let's install torchsummary and do some cool stuff \n",
        "!pip install torchsummary # https://github.com/sksq96/pytorch-summary "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NylWMGTf1QbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUFlNAd5lhN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "4fd134d5-e359-4179-9a6b-44c78ff7d5c4"
      },
      "source": [
        "# We can make the use of torchsummary library here to figure \n",
        "# if we have done something wrong \n",
        "\n",
        "# But first we need to tell PyTorch where to 'keep' the model \n",
        "# On GPU or on CPU \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # ---- FILL IN ----\n",
        "print('The model will run on', device)\n",
        "\n",
        "# Initialize the model \n",
        "mnist1 = MNISTModel1().to(device) # ---- FILL IN ----\n",
        "summary(model=mnist1, input_size=(1, 28, 28), batch_size=20 ) # ---- FILL IN ----) # Summarize"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model will run on cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [20, 8, 28, 28]              80\n",
            "            Conv2d-2           [20, 16, 28, 28]           1,168\n",
            "            Linear-3                  [20, 256]       3,211,520\n",
            "            Linear-4                   [20, 64]          16,448\n",
            "           Dropout-5                   [20, 64]               0\n",
            "            Linear-6                   [20, 10]             650\n",
            "================================================================\n",
            "Total params: 3,229,866\n",
            "Trainable params: 3,229,866\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 2.93\n",
            "Params size (MB): 12.32\n",
            "Estimated Total Size (MB): 15.31\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyTvkdq3gqHM",
        "colab_type": "text"
      },
      "source": [
        "That was a lot of work.....Whew!  \n",
        "\n",
        "_***Q: Isn't there an 'easier' way to define the Model in 2020?***_  \n",
        "A: _**Yes, absolutely!**_  \n",
        "Say hello to the _**[torchlayers library !!!](https://github.com/szymonmaszke/torchlayers \"The best thing to have happened to PyTorch in recent times, shame it won't run on Colab!\")**_  \n",
        "With torchlayers, the above code will be reduced to about 7-8 lines!  \n",
        "But unfortunately, [torchlayers requires Python 3.7](https://github.com/szymonmaszke/torchlayers/issues/5 \"I tried my best!\") and above. Colab only  \n",
        "supports Python 3.6.x.  \n",
        "```\n",
        "# A slightly different example from ours\n",
        "import torchlayers as tl\n",
        "\n",
        "# torch.nn and torchlayers can be mixed easily\n",
        "model = torch.nn.Sequential(\n",
        "    tl.Conv(64),  # specify ONLY out_channels\n",
        "    torch.nn.ReLU(),  # use torch.nn wherever you wish\n",
        "    tl.BatchNorm(),  # BatchNormNd inferred from input\n",
        "    tl.Conv(128),  # Default kernel_size equal to 3\n",
        "    tl.ReLU(),\n",
        "    tl.Conv(256, kernel_size=11),  # \"same\" padding as default\n",
        "    tl.GlobalMaxPool(),  # Known from Keras\n",
        "    tl.Linear(10),  # Output for 10 classes\n",
        ")\n",
        "\n",
        "print(model)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YljS2tiRlHql",
        "colab_type": "text"
      },
      "source": [
        "With our model definition complete, it is time to train! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f08VsPbPn5bO",
        "colab_type": "text"
      },
      "source": [
        "### 5. Train a PyTorch Model in 2020  \n",
        "\n",
        "PyTorch is infamous among newcomers for it's _'training loops'_.  \n",
        "They can be long and at times a little confusing too. However, most of them   \n",
        "are similar and writing training loops simply turns out to be a boring and repetitive  \n",
        "exercise.  \n",
        "\n",
        "Q: _**This is 2020.**_ Is there a better way?  \n",
        "A: _**Yes, absolutely!**_  \n",
        "Say hello to _**[Poutyne !!!](https://poutyne.org/index.html \"How do you pronounce this?\")**_   \n",
        "\n",
        "\n",
        "Thanks to _Poutyne_, writing training loops in PyTorch is _**FUN !!!**_  \n",
        "\n",
        "PS - Poutyne is pronounced as Poutine or Pu-tin.  \n",
        "![Poutyne](https://drive.google.com/uc?id=142xYy_mJoPSk97SDicvn9zRNxHMpXDxz \"You think loops are boring?!?!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gpy_1cRZ6gV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "965584eb-f5ce-49a0-b3e6-9f3fc345a8dd"
      },
      "source": [
        "# Install Poutyne \n",
        "!pip install poutyne "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: poutyne in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.18.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnHdjqkhdzLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from poutyne.framework import Model # The core datastructure of poutyne \n",
        "                                    # https://poutyne.org/model.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHVhp0yEiFfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim # Optimizer: we need it to train our network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EwMBpBtKE7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A pouytne training loop\n",
        "\n",
        "# A few hyperparamters for the training loop \n",
        "learning_rate = 0.1\n",
        "epochs = 3\n",
        "\n",
        "def poutyne_train(pytorch_model):\n",
        "    \n",
        "    # ---- FILL IN ----\n",
        "    optimizer = optim.SGD(pytorch_model.parameters(), lr=learning_rate)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    model = Model(pytorch_model, optimizer, loss_function,\n",
        "                  batch_metrics=['accuracy'])\n",
        "    \n",
        "    model.to(device)\n",
        "    #this  will train th emodel\n",
        "    model.fit_generator(train_loader, valid_loader, epochs=epochs)\n",
        "\n",
        "    #Test on the Test set\n",
        "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "    \n",
        "    print(f'Test:\\n\\tLoss: {test_loss: .3f}\\n\\tAccuracy: {test_acc: .3f}')\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_znXT5djwzeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "0fe69c94-d68b-48aa-f3a4-b3aeee9749c8"
      },
      "source": [
        "# Let's start the training people!!! \n",
        "poutyne_train(mnist1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3 23.55s Step 2400/2400: loss: 0.338168, acc: 89.510417, val_loss: 0.120862, val_acc: 96.341667\n",
            "Epoch 2/3 23.40s Step 2400/2400: loss: 0.083483, acc: 97.520833, val_loss: 0.071190, val_acc: 97.800000\n",
            "Epoch 3/3 23.46s Step 2400/2400: loss: 0.050254, acc: 98.493750, val_loss: 0.057918, val_acc: 98.341667\n",
            "Test:\n",
            "\tLoss:  0.030\n",
            "\tAccuracy:  99.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqforBpLr0dk",
        "colab_type": "text"
      },
      "source": [
        "_**CONGRATULATIONS !!! You just trained your first(?) CNN Model!**_   \n",
        "The accuracy looks pretty decent as well! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hakTX49k5wnW",
        "colab_type": "text"
      },
      "source": [
        "### 6. Understanding Overfitting \n",
        "Before we move any further we need to understand the concept of _**Overfitting**_  \n",
        "in Machine Learning models.  \n",
        "What is _**overfitting?**_  \n",
        "![Fitting Examples](https://drive.google.com/uc?id=1gFOa5I24S7XDDep4WaVMIoivh9JvIp1Y \"https://www.curiousily.com/posts/hackers-guide-to-fixing-underfitting-and-overfitting-models/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnE0htQzVRAs",
        "colab_type": "text"
      },
      "source": [
        "We are always looking to ensure that our models have a low bias with a low  \n",
        "variance.  \n",
        "However, if we train for too long (too many epochs) our model will start to  \n",
        "overfit.  \n",
        "\n",
        "How do we identify that the model has started overfitting?  \n",
        "![Overfitting](https://drive.google.com/uc?id=1q02q0ge0jldHJm8P_Hq6ECeIvaSlcc36 \"https://mlexplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/\")  \n",
        "Dotted vertical line is where we should either stop training the model (also  \n",
        "known as _**Early Stopping**_) or we should have some logic in the training  \n",
        "loop that _'saves'_ the model around that time while the training still  \n",
        "continues. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6qwm4NIZQNf",
        "colab_type": "text"
      },
      "source": [
        "Our model will also overfit if it is too complex.  \n",
        "For example (y-axis is different),  \n",
        "![Complex Model](https://drive.google.com/uc?id=1vWHgknPrbXEQczdKMliB0RyVgYld3cGa \"https://medium.com/@george.drakos62/cross-validation-70289113a072\")  \n",
        "\n",
        "<br/>Early stopping can save the day!  \n",
        "![alt text](https://drive.google.com/uc?id=1HnBFMWZGHy0UFMUICKU25qBiB-4_G0Xn \"https://www.jeremyjordan.me/deep-neural-networks-preventing-overfitting/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhvAC7xGapnU",
        "colab_type": "text"
      },
      "source": [
        "A really nice matrix to identify if we are overfitting:  \n",
        "![Overfitting Matrix](https://drive.google.com/uc?id=19bFHepjNgQ9kpqmQdMEW4lDJ-qXi5WF8 \"https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42\")  \n",
        "While preparing the above matrix, the author has considered only two sets,  \n",
        "training and testing (kinda Deep Learning _faux pass_!). Since we have a  \n",
        "validation set as well, replace the word _**'Testing'**_ with  \n",
        "_**'Validation'**_. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qlOIi5RaeVI",
        "colab_type": "text"
      },
      "source": [
        "How can we avoid _**Overfitting**_?  \n",
        "1. Use a simpler model (less layers)  \n",
        "2. Use _Dropout_ \n",
        "3. Get more training data (if possible) \n",
        "4. Augment the data and add noise  \n",
        "5. Early Stopping  \n",
        "\n",
        "PS - Not an exhaustive list AT ALL.  \n",
        "\n",
        "<br/>Can we write a better training loop now? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10dxUCy3iR0A",
        "colab_type": "text"
      },
      "source": [
        "### 8. A Better Training Loop  \n",
        "We combine our knowledge of Early Stopping and saving the model at the right  \n",
        "time during training to write a better _Training Loop_.  \n",
        "\n",
        "We use [Callbacks](https://poutyne.org/callbacks.html# \"Click to visit link\") in Poutyne to incorporate Early Stopping and saving the model after every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLajWBECiXsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from poutyne.framework import ModelCheckpoint # Saves trained model during training\n",
        "                                              # https://poutyne.org/callbacks.html#checkpointing\n",
        "from poutyne.framework import EarlyStopping # You know what it does! ;) \n",
        "                                            # https://poutyne.org/callbacks.html#poutyne.framework.callbacks.EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icK1WbNHr-ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A better pouytne training loop\n",
        "# Turn the GPU ON\n",
        "\n",
        "# A few hyperparamters for the training loop \n",
        "learning_rate = 0.1\n",
        "epochs = 10 # let's train for more epochs to see the callbacks in action\n",
        "\n",
        "def better_poutyne_train(model_name, pytorch_model):\n",
        "    \n",
        "    callbacks = [\n",
        "        # ---- FILL IN ----\n",
        "        ModelCheckpoint(model_name+'_last_epoc.ckpt', \\\n",
        "                        temporary_filename='last_epoch.ckpt.tmp'),\n",
        "        #Early Stopping\n",
        "        EarlyStopping(monitor='val_acc',patience=0, verbose=True, mode='max')\n",
        "    ]\n",
        "    \n",
        "    # Select the optimizer and the loss function \n",
        "    optimizer = optim.SGD(pytorch_model.parameters(), lr=learning_rate)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    # Poutyne Model\n",
        "    model = Model(pytorch_model, optimizer, loss_function, batch_metrics=['accuracy'])\n",
        "    # Send the 'Poutyne model' on GPU/CPU whichever is available \n",
        "    model.to(device)\n",
        "    # Train\n",
        "    model.fit_generator(train_loader, valid_loader, epochs=epochs, callbacks=callbacks)\n",
        "    # Test\n",
        "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "    print(f'Test:\\n\\tLoss: {test_loss: .3f}\\n\\tAccuracy: {test_acc: .3f}')\n",
        "\n",
        "    return None "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrxuV06YE2QV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e1cae556-cbaa-4d09-a37d-e05426bb965b"
      },
      "source": [
        "# COLAB TIP\n",
        "!nvidia-smi # Use this command to figure out the GPU assigned by Google  "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 12 20:36:49 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    32W / 250W |    769MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffcjuFHa5v75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f71e5484-4324-4560-f1a5-570591742675"
      },
      "source": [
        "# Let's initialize a new CNN Model just like before \n",
        "# ---- FILL IN ----\n",
        "mnist2 =  MNISTModel1().to(device)\n",
        "summary(model=mnist2, input_size=(1, 28, 28), batch_size=20)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [20, 8, 28, 28]              80\n",
            "            Conv2d-2           [20, 16, 28, 28]           1,168\n",
            "            Linear-3                  [20, 256]       3,211,520\n",
            "            Linear-4                   [20, 64]          16,448\n",
            "           Dropout-5                   [20, 64]               0\n",
            "            Linear-6                   [20, 10]             650\n",
            "================================================================\n",
            "Total params: 3,229,866\n",
            "Trainable params: 3,229,866\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 2.93\n",
            "Params size (MB): 12.32\n",
            "Estimated Total Size (MB): 15.31\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTWPxL4Y6Avb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c688b6c1-9f51-40cd-b396-f0a7ab7673e0"
      },
      "source": [
        "# Time to train in a better way!.....takes about 120secs on GPU \n",
        "# Make sure to Turn the GPU ON\n",
        "# ---- FILL IN ----\n",
        "better_poutyne_train(model_name='mnist2', pytorch_model=mnist2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 ETA 2s Step 2209/2400: loss: 0.423914, acc: 80.000000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us0ZQ7nqBGUk",
        "colab_type": "text"
      },
      "source": [
        "Nice! But you showed us those fancy graphs while talking about early stopping.  \n",
        "_**Where are those graphs now?!**_  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYsQWympCOuv",
        "colab_type": "text"
      },
      "source": [
        "###9. A Fancy Training Loop  \n",
        "\n",
        "When it comes to productivity enhancing libraries in the PyTorch ecosystem  \n",
        "Poutyne is not alone.  \n",
        "Say hello to [LiveLossPlot !!!](https://github.com/stared/livelossplot \"Click to visit Github Repository\")  \n",
        "\n",
        "Best part is the fact that LiveLossPlot and Poutyne are compatible with each  \n",
        "other! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uiu2yYMU6hYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install livelossplot "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXlPkpKsTsfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from livelossplot import PlotLossesPoutyne # This module talks with Poutyne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLpRKgNOGWFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A fancy pouytne training loop\n",
        "# Turn the GPU ON\n",
        "\n",
        "# A few hyperparamters for the training loop \n",
        "learning_rate = 0.1\n",
        "epochs = 10 # let's train for more epochs to see the callbacks in action\n",
        "\n",
        "def fancy_poutyne_train(model_name, pytorch_model):\n",
        "    \n",
        "    # setting up the livelossplot callback\n",
        "    # ---- FILL IN ----\n",
        "    plotlosses = PlotLossesPoutyne()\n",
        "\n",
        "    callbacks = [\n",
        "        # Save the latest weights \n",
        "        ModelCheckpoint(model_name + '_last_epoch.ckpt', \\\n",
        "                        temporary_filename='last_epoch.ckpt.tmp'),\n",
        "        # EarlyStopping\n",
        "        EarlyStopping(monitor='val_acc', patience=0, verbose=True, mode='max'),\n",
        "        # ---- FILL IN ----\n",
        "        plotlosses\n",
        "    ]\n",
        "    \n",
        "    # Select the optimizer and the loss function \n",
        "    optimizer = optim.SGD(pytorch_model.parameters(), lr=learning_rate)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    # Poutyne Model\n",
        "    model = Model(pytorch_model, optimizer, loss_function, batch_metrics=['accuracy'])\n",
        "    # Send the 'Poutyne model' on GPU/CPU whichever is available \n",
        "    model.to(device)\n",
        "    # Train\n",
        "    model.fit_generator(train_loader, valid_loader, epochs=epochs, callbacks=callbacks)\n",
        "    # Test\n",
        "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "    print(f'Test:\\n\\tLoss: {test_loss: .3f}\\n\\tAccuracy: {test_acc: .3f}')\n",
        "\n",
        "    return None "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj9sG_AUUShG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Yet another CNN Model \n",
        "mnist3 = MNISTModel1().to(device)\n",
        "summary(model=mnist3, input_size=(1, 28, 28), batch_size=20) # Summarize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwA8L0XWUbVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let the Fancy training loop start! \n",
        "# Make sure to Turn the GPU ON\n",
        "# ---- FILL IN ----\n",
        "fancy_poutyne_train(model_name='mnist3', pytorch_model=mnist3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxpV5o0Z-b3G",
        "colab_type": "text"
      },
      "source": [
        "Thanks to LiveLossPlot, we can see all the fancy graphs now!  \n",
        "\n",
        "_**You can now do a little experiment and see for yourself how the model starts  \n",
        "to overfit. Simply remove the Early Stopping callback from the training loop.**_\n",
        "\n",
        "<br/>You can see your model overfit even on such a simple dataset! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaXagq-rA8vL",
        "colab_type": "text"
      },
      "source": [
        "###10. Inference  \n",
        "We have a trained model in our hands now.  \n",
        "We would now like to write a simnple inference routine where we can enjoy  \n",
        "the predictions of our PyTorch Model! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpqrJYFZUpig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A rather straightforward inference routine \n",
        "\n",
        "def inference():\n",
        "    PATH = 'mnist3_last_epoch.ckpt' # Path to the saved model checkpoint\n",
        "\n",
        "    # ---- FILL IN ----\n",
        "    model = MNISTModel1().to('cpu')\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "    #summary(model, (1,28,28), device='cpu')\n",
        "\n",
        "    dataiter = iter(train_loader)\n",
        "    images, lables = dataiter.next()\n",
        "    label = lables[0]\n",
        "\n",
        "    image = images[0]\n",
        "    image = torch.unsqueeze(image, dim=0)\n",
        "    print('\\nimage.shape =>' , image.shape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = model.forward(image)\n",
        "    \n",
        "    # Our model outputs 'logits', we need to transform it into class probabilities. \n",
        "    # https://discuss.pytorch.org/t/how-to-extract-probabilities/2720/12\n",
        "    # To transform logits, we need to use the 'Softmax' function\n",
        "    # https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
        "    # Therefore,  \n",
        "    class_probabilities = F.softmax(output, dim=1).numpy().squeeze()\n",
        "    print('\\nClass Probabilities ==>', class_probabilities)\n",
        "    for i, proba in enumerate(class_probabilities):\n",
        "        print(f'Class \\t{i}\\t Probability \\t{100*proba:.2f}%')\n",
        "\n",
        "    # A very Fancy way to showcase the results \n",
        "    # Create a figure with two axes, ax1 and ax2\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2) # Subplot with 2 columns\n",
        "    # ax1 holds the image from the test dataset\n",
        "    ax1.imshow(image.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.set_title('Ground Truth ' + str(label.numpy()))\n",
        "    # ax2 holds a horizontal bar chart containing class_probabilities \n",
        "    ax2.barh(np.arange(10), class_probabilities)\n",
        "    ax2.set_aspect(0.1) # aspect ratio of ax2, else it will get too big\n",
        "    ax2.set_yticks(np.arange(10)) # 10 ticks on the y-axis for 10 classes\n",
        "    ax2.set_yticklabels(np.arange(10)) # set the ticklabels from 0 to 9\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1) # probability can't be over 1, hence set limit to 1.1\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enklrq5uH3qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inference() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAttJPwQ8O4t",
        "colab_type": "text"
      },
      "source": [
        "_**CONGRATULATIONS!!!**_  \n",
        "You just wrote a complete inference loop for your Deep Learning Model!  \n",
        "\n",
        "![Congratulations](https://media.giphy.com/media/F22UTGzxpASWc/source.gif \"Can you identify the guitarist? ;) \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKN3ZR_Gh22S",
        "colab_type": "text"
      },
      "source": [
        "###11. Thank You!  \n",
        "Guys, thank you so much for being with me through this webinar.  \n",
        "I would like to thank each and every one of you out there!  \n",
        "\n",
        "I will be really happy to connect with you all on LinkedIN, feel free to drop  \n",
        "in a connection request. \n",
        " \n",
        "https://www.linkedin.com/in/pranjall/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2lXpTfRk7JO",
        "colab_type": "text"
      },
      "source": [
        "###12. BONUS! \n",
        "\n",
        "I will be adding some bonus content in this Notebook in the coming week.  \n",
        "Namely,  \n",
        "1. Transfer Learning  \n",
        "2. Visualizing computation graphs of your custom model  \n",
        "3. Visualizing what your CNN layer is looking at (it is amazing, trust me!)  \n",
        "\n",
        "Consider _**starring**_ this repository on Github if you liked this webinar  \n",
        "and want to get the bonus material coming later in this week.  \n",
        "A lot of my peers at Udacity have loved the content in this repo, checkout the  \n",
        "`Introduction to Neural Networks` folder.  \n",
        "\n",
        "I plan to keep uploading/updating the content of all of my future webinars in   \n",
        "this repository. \n",
        "\n",
        "https://github.com/pranjalchaubey/Deep-Learning-Notes  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KbNEwlOmLq1",
        "colab_type": "text"
      },
      "source": [
        "_**Wishing you guys all the best in your Deep Learning journey with PyTorch!**_ "
      ]
    }
  ]
}